{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5de4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing the dataset\n",
    "salary_data = pd.read_csv('kaggle_survey_2020_responses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e0133f",
   "metadata": {},
   "source": [
    "## Q11,12,14 and 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe6bdc",
   "metadata": {},
   "source": [
    "#### New DataFrame containg just the columns corresponding to these questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d444af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = salary_data.iloc[:,47:52]\n",
    "df_2 = salary_data.iloc[:,53:66]\n",
    "\n",
    "df_q11_12_14_15 = pd.concat([df_1, df_2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075af9c1",
   "metadata": {},
   "source": [
    "#### Cleaning and encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b1bd1",
   "metadata": {},
   "source": [
    "### Q11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41b3032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column has 3007 null values\n"
     ]
    }
   ],
   "source": [
    "print(f\"The column has {df_q11_12_14_15['q11'].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f123aa",
   "metadata": {},
   "source": [
    "When we observe the data and the response choices for this question, it is evident that null values (associated with no response - empty entry in the datasheet) imply that the person didn't specify any platform. Thus, we shall fill these null values with 'mode' for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e608d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q11_12_14_15['q11'].fillna(df_q11_12_14_15['q11'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976fc47",
   "metadata": {},
   "source": [
    "Distribution of the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1c14de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A personal computer or laptop                                          16355\n",
       "A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)     2358\n",
       "A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)                834\n",
       "None                                                                     292\n",
       "Other                                                                    197\n",
       "Name: q11, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q11_12_14_15['q11'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd3e70",
   "metadata": {},
   "source": [
    "#### Method for encoding\n",
    "\n",
    "##### Motivation\n",
    "Since this column has 5 categories we could one-hot encode this column but since we are dealing with multiple questions which have several unique categories, plus some of the observations in those columns have multiple categories as their value thus we would have to resort to one-hot encoding for such columns, thus, we believe that it would be reasonable to use an encoding technique that preserves the relation between the categories and our target variable and Target Encoding seems like a viable option. But the drawback with Target Encoding is target leakage since the target is used to predict the target. Such models tend to be overfitted and donâ€™t generalize well in unseen circumstances.\n",
    "\n",
    "Thus we can do one better than this and opt for CatBoost encoding. A CatBoost encoder is similar to target encoding, but also involves an ordering principle in order to overcome the problem of target leakage. It uses the principle similar to the time series data validation. The values of target statistic rely on the observed history, i.e, target probability for the current feature is calculated only from the rows (observations) before it.\n",
    "\n",
    "Note: This may lead to a problem if our observations have an inherent ordering but since there is no inherent ordering in our data observations we already have a randomly shuffled dataset and thus CatBoost encoder should do a fine job.\n",
    "\n",
    "We shall be implementing this technique once we have split our dataset into development and test in the later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a01015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "\n",
    "#from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "#q11_encoder = CatBoostEncoder()\n",
    "#X_dev['q11'] = q11_encoder.fit_transform(X_dev['q11'], y_dev)\n",
    "#X_test['q11'] = q11_encoder.transform(X_test['q11'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56091c37",
   "metadata": {},
   "source": [
    "### Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3080d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The q12_part_1 column has 11726 null values\n",
      "The q12_part_2 column has 19076 null values\n",
      "The q12_part_3 column has 12145 null values\n",
      "The q12_other column has 19370 null values\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    print(f\"The {df_q11_12_14_15.columns[i]} column has {df_q11_12_14_15.iloc[:,i].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5d95f",
   "metadata": {},
   "source": [
    "These values don't signify missing data but in fact what we have in actuality is sparse data. Thus, we shall fill these null values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e867ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    df_q11_12_14_15[df_q11_12_14_15.columns[i]].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47074947",
   "metadata": {},
   "source": [
    "#### Method for encoding\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "For this particular question, the most suitable encoding technique would again be One-Hot-Encoding since each of the observation can have multiple choices from the available options and through understanding of the problem statement, we feel that this might be an important feature and our goal would be to prevent any data leakage at all.\n",
    "\n",
    "To carry this out for this dataset is fairly easy since the dataset in itself has a seperate column for each of the options within the question. All we need to do is simply convert the string data in each of the columns to 1 given we have already assigned to 0 to samples that didn't have the particular option selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d16742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding for Question 12\n",
    "\n",
    "for i in range(1, 5):\n",
    "    df_q11_12_14_15[df_q11_12_14_15.columns[i]].mask(df_q11_12_14_15[df_q11_12_14_15.columns[i]] != 0, 1 , inplace=True )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18351fb",
   "metadata": {},
   "source": [
    "The column 'q12_part_3' which corresponds to the choice 'None' doesn't really add much information thus we should drop it from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c45dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q11_12_14_15.drop('q12_part_3', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcb072",
   "metadata": {},
   "source": [
    "### Q14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e1196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The q14_part_1 column has 7694 null values\n",
      "The q14_part_2 column has 11215 null values\n",
      "The q14_part_3 column has 15906 null values\n",
      "The q14_part_4 column has 15916 null values\n",
      "The q14_part_5 column has 18899 null values\n",
      "The q14_part_6 column has 19207 null values\n",
      "The q14_part_7 column has 19803 null values\n",
      "The q14_part_8 column has 19116 null values\n",
      "The q14_part_9 column has 19186 null values\n",
      "The q14_part_10 column has 19440 null values\n",
      "The q14_part_11 column has 18139 null values\n",
      "The q14_other column has 19471 null values\n"
     ]
    }
   ],
   "source": [
    "for i in range(4, 16):\n",
    "    print(f\"The {df_q11_12_14_15.columns[i]} column has {df_q11_12_14_15.iloc[:,i].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd083b7d",
   "metadata": {},
   "source": [
    "Again, these values don't signify missing data but in fact what we have in actuality is sparse data. Thus, we shall fill these null values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f46def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4, 16):\n",
    "    df_q11_12_14_15[df_q11_12_14_15.columns[i]].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a4317",
   "metadata": {},
   "source": [
    "#### Method for encoding\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "For this particular question, the most suitable encoding technique would again be One-Hot-Encoding since each of the observation can have multiple choices from the available options and through understanding of the problem statement, we feel that this might be an important feature and our goal would be to prevent any data leakage at all. We implement this in te same way as we did for Q12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb6a68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding for Question 14\n",
    "\n",
    "for i in range(4, 16):\n",
    "    df_q11_12_14_15[df_q11_12_14_15.columns[i]].mask(df_q11_12_14_15[df_q11_12_14_15.columns[i]] != 0, 1 , inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4664645c",
   "metadata": {},
   "source": [
    "Again, column 'q14_part_11' which corresponds to the choice 'None' doesn't really add much information thus we should drop it from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6baa408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q11_12_14_15.drop('q14_part_11', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86e219",
   "metadata": {},
   "source": [
    "### Q15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1fe707b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The column has 3662 null values\n"
     ]
    }
   ],
   "source": [
    "print(f\"The column has {df_q11_12_14_15['q15'].isnull().sum()} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781cefe1",
   "metadata": {},
   "source": [
    "For this question, we will be interpreting the null values (associated with no response - empty entry in the datasheet) to be no experience in using machine learning methods, thus we will impute the missing values with 'I do not use machine learning methods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c565abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q11_12_14_15['q15'].fillna(\"I do not use machine learning methods\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fece780d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Under 1 year                             6312\n",
       "I do not use machine learning methods    5737\n",
       "1-2 years                                3459\n",
       "2-3 years                                1631\n",
       "3-4 years                                 893\n",
       "5-10 years                                801\n",
       "4-5 years                                 784\n",
       "10-20 years                               244\n",
       "20 or more years                          175\n",
       "Name: q15, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q11_12_14_15['q15'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225d4e1",
   "metadata": {},
   "source": [
    "#### Method for encoding\n",
    "\n",
    "##### Motivation\n",
    "\n",
    "Since data values are representing a range of values, we may define it as the mean of the range but the problem arises with the last category because it states 20 or more years and we have no information on the upperbound. Another technique would be to one-hot encode which makes a lot of sense but again, we have to keep in mind that we are already dealing with high-dimensional data and adding more dimensions will only make it difficult for us. But, if we observe more keenly, this column appears to have some ordinaliity since we may explicitly define an order as follows:\n",
    "\n",
    "    1. I do not use machine learning methods\n",
    "    2. Under 1 year\n",
    "    3. 1-2 years\n",
    "    4. 2-3 years\n",
    "    5. 3-4 years\n",
    "    6. 4-5 years\n",
    "    7. 5-10 years\n",
    "    8. 10-20 years\n",
    "    9. 20 or more years\n",
    "    \n",
    "Thus, we shall be performing one-hot encoding for this column. This will be implemented once we have split our dataset into development and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5e7b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "\n",
    "#from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#q15_enc = OrdinalEncoder(categories=['I do not use machine learning methods','Under 1 year','1-2 years','2-3 years','3-4 years','4-5 years', '5-10 years', '10-20 years', '20 or more years'])\n",
    "#X_dev['q15'] = q15_enc.fit_transform(X_dev['q15'])\n",
    "#X_test['q15'] = q15_enc.transform(X_test['q15']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
